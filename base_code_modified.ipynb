{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7f0b5ce-2ac1-41e5-9b1e-b445e0d2e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import envs\n",
    "#from colabgymrender.recorder import Recorder\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import ResNet50,MobileNetV2\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a5b0c21-33a2-40b2-a153-a921d677ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model=MobileNetV2(input_shape=(160,160,3),           \n",
    "                    include_top=False,\n",
    "                    weights='imagenet') \n",
    "\n",
    "for layer in trained_model.layers:\n",
    "    layer.trainable=False\n",
    "#trained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ccfe604-d24d-40a4-b49c-d3fb5abb80a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "seed = 42\n",
    "env.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "#env._max_episode_steps = 1000\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "  def __init__(self,mod):\n",
    "    super(Model, self).__init__()\n",
    "    self.mod = mod\n",
    "    self.inputs = layers.Input(shape=(160,160,3))\n",
    "    self.l1 = layers.GlobalAveragePooling2D()\n",
    "    self.actor = layers.Dense(6,activation='linear')\n",
    "    self.critic = layers.Dense(1,activation='linear')\n",
    "\n",
    "  def call(self,inp):\n",
    "    #x = self.inputs(inp)\n",
    "    x = self.mod.predict(inp)\n",
    "    x = self.l1(x)\n",
    "    x1 = self.actor(x)\n",
    "    y1 = self.critic(x)\n",
    "\n",
    "    return [x1,y1]\n",
    "\n",
    "model = Model(trained_model)\n",
    "\n",
    "\n",
    "# def create_model(mod):\n",
    "#   inputs = layers.Input(shape=(160,160,3))\n",
    "#   x = mod(inputs,training=False)\n",
    "#   x1 = layers.GlobalAveragePooling2D()(x)\n",
    "#   actor = layers.Dense(env.action_space.n,activation='linear')(x1)\n",
    "#   critic = layers.Dense(1,activation = 'linear')(x1)\n",
    "\n",
    "#   return keras.Model(inputs=inputs, outputs=[actor,critic])\n",
    "\n",
    "# model = create_model(trained_model)\n",
    "loss_fn = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bf8e0d9-1770-4bba-a4ac-2895f57481e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "\n",
    "  r = np.array(r)\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  gamma = 0.99\n",
    "\n",
    "  for t in range(r.size-2,-1,-1):\n",
    "    discounted_r[t] = r[t]+discounted_r[t+1]*gamma\n",
    "  discounted_r -= np.mean(discounted_r) #normalizing the result\n",
    "  discounted_r /= (np.std(discounted_r)+eps) #idem using standar deviation\n",
    "  return discounted_r.astype(np.float32)\n",
    "\n",
    "def compute_loss(action_probs,values,returns):\n",
    "\n",
    "  advantage = returns - values\n",
    "  action_log_probs = tf.math.log(action_probs)\n",
    "  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
    "  critic_loss = loss_fn(values, returns)\n",
    "\n",
    "  return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762d18bf-449d-4e64-9305-ce3a2176b48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode reward -21.0 reward -21.0 episode: 1 frame: 683\n",
      "episode reward -21.0 reward -21.0 episode: 2 frame: 1392\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_rewards = 0\n",
    "frame_count = 0\n",
    "episode_count = 0\n",
    "gamma = 0.99\n",
    "episode_reward_history = []\n",
    "\n",
    "for i in range(300):\n",
    "  state = np.array(env.reset())\n",
    "  rewards=[]\n",
    "  action_probs = []\n",
    "  values = []\n",
    "  done=False\n",
    "  with tf.GradientTape() as tape:\n",
    "    while not done:\n",
    "      frame_count+=1\n",
    "      state = state[:160,:,:]\n",
    "      state_tensor = tf.convert_to_tensor(state)\n",
    "      state_tensor = tf.expand_dims(state_tensor,0)\n",
    "      action_prob,value = model(state_tensor)\n",
    "      action = tf.random.categorical(action_prob, 1)[0, 0]\n",
    "      action = action.numpy()\n",
    "      action_probs_t = tf.nn.softmax(action_prob)\n",
    "\n",
    "      state_next,reward,done,info = env.step(action)\n",
    "\n",
    "      rewards.append(reward)\n",
    "      action_probs.append(action_probs_t[0][action])\n",
    "      values.append(tf.squeeze(value))\n",
    "\n",
    "      state = np.array(state_next)\n",
    "\n",
    "      if done:\n",
    "        break\n",
    "\n",
    "\n",
    "    returns = discount_rewards(rewards)\n",
    "    action_probs, values, returns = [tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \n",
    "    loss = compute_loss(action_probs,values,returns)\n",
    "    loss = tf.convert_to_tensor([loss])\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "\n",
    "  episode_reward_history.append(sum(rewards))\n",
    "  if len(episode_reward_history)>100:\n",
    "    del episode_reward_history[:1]\n",
    "  total_rewards = np.mean(episode_reward_history)\n",
    "\n",
    "  episode_count+=1\n",
    "  #if episode_count%5==0:\n",
    "  print(\"episode reward\",sum(rewards),\"reward\",total_rewards,\"episode:\",episode_count,\"frame:\",frame_count)\n",
    "  if total_rewards>19:\n",
    "    print(\"The environment was solved at episode:\",episode_count)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b57a66-8ab9-4d0b-bbba-603eaf981d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iisc:Python",
   "language": "python",
   "name": "conda-env-iisc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

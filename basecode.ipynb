{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMEp2XegQNh72n51hYKYSVM"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"vfgXt27-EoM0"},"source":["!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","!pip install colabgymrender==1.0.2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AOLgqQh2Et7m"},"source":["! wget http://www.atarimania.com/roms/Roms.rar\n","! mkdir /content/ROM/\n","! unrar e /content/Roms.rar /content/ROM/\n","! python -m atari_py.import_roms /content/ROM/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"euUz8caLEwlJ"},"source":["# !pip3 install Box2D\n","# !pip3 install box2d-py\n","# !pip3 install gym[Box_2D]\n","import gym\n","from gym import envs\n","from colabgymrender.recorder import Recorder\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import numpy as np\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.applications import ResNet50\n","from tensorflow.keras.models import Sequential"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eQ2j4CnhEpKB"},"source":["d=envs.registry.all()\n","for i in d:\n","  print(i)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1W9ZPwGzyHOX"},"source":["trained_model=ResNet50(input_shape=(160,160,3),           \n","                    include_top=False,\n","                    weights='imagenet') \n","\n","for layer in trained_model.layers:\n","    layer.trainable=False\n","trained_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wPxQwC9sU9Hk"},"source":["env = gym.make('Pong-v0')\n","env = gym.wrappers.ResizeObservation(env,(160,160))\n","seed = 42\n","env.seed(seed)\n","tf.random.set_seed(seed)\n","np.random.seed(seed)\n","#env._max_episode_steps = 1000\n","eps = np.finfo(np.float32).eps.item()\n","\n","def create_model(mod):\n","  inputs = layers.Input(shape=(160,160,3))\n","  x = mod(inputs,training=False)\n","  x1 = layers.GlobalAveragePooling2D()(x)\n","  actor = layers.Dense(env.action_space.n,activation='linear')(x1)\n","  critic = layers.Dense(1,activation = 'linear')(x1)\n","\n","  return keras.Model(inputs=inputs, outputs=[actor,critic])\n","\n","model = create_model(trained_model)\n","loss_fn = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n","optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BWEFwrKrVRMv"},"source":["def discount_rewards(r):\n","\n","  r = np.array(r)\n","  discounted_r = np.zeros_like(r)\n","  running_add = 0\n","  gamma = 0.99\n","\n","  for t in range(r.size-2,-1,-1):\n","    discounted_r[t] = r[t]+discounted_r[t+1]*gamma\n","  discounted_r -= np.mean(discounted_r) #normalizing the result\n","  discounted_r /= (np.std(discounted_r)+eps) #idem using standar deviation\n","  return discounted_r.astype(np.float32)\n","\n","def compute_loss(action_probs,values,returns):\n","\n","  advantage = returns - values\n","  action_log_probs = tf.math.log(action_probs)\n","  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n","  critic_loss = loss_fn(values, returns)\n","\n","  return actor_loss + critic_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A0GmQ8az0rXq"},"source":["\n","total_rewards = 0\n","frame_count = 0\n","episode_count = 0\n","gamma = 0.99\n","episode_reward_history = []\n","\n","for i in range(300):\n","  state = np.array(env.reset())\n","  rewards=[]\n","  action_probs = []\n","  values = []\n","  done=False\n","  with tf.GradientTape() as tape:\n","    while not done:\n","      frame_count+=1\n","      \n","      state_tensor = tf.convert_to_tensor(state)\n","      state_tensor = tf.expand_dims(state_tensor,0)\n","      action_prob,value = model(state_tensor)\n","      action = tf.random.categorical(action_prob, 1)[0, 0]\n","      action = action.numpy()\n","      action_probs_t = tf.nn.softmax(action_prob)\n","\n","      state_next,reward,done,info = env.step(action)\n","\n","      rewards.append(reward)\n","      action_probs.append(action_probs_t[0][action])\n","      values.append(tf.squeeze(value))\n","\n","      state = np.array(state_next)\n","\n","      if done:\n","        break\n","\n","\n","    returns = discount_rewards(rewards)\n","    action_probs, values, returns = [tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \n","    loss = compute_loss(action_probs,values,returns)\n","    loss = tf.convert_to_tensor([loss])\n","  grads = tape.gradient(loss, model.trainable_variables)\n","  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","\n","  episode_reward_history.append(sum(rewards))\n","  if len(episode_reward_history)>100:\n","    del episode_reward_history[:1]\n","  total_rewards = np.mean(episode_reward_history)\n","\n","  episode_count+=1\n","  #if episode_count%5==0:\n","  print(\"episode reward\",sum(rewards),\"reward\",total_rewards,\"episode:\",episode_count,\"frame:\",frame_count)\n","  if total_rewards>19:\n","    print(\"The environment was solved at episode:\",episode_count)\n","    break"],"execution_count":null,"outputs":[]}]}
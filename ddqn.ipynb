{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2051768d-3f59-4ebd-ac54-37d7e539492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import envs\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import ResNet50,MobileNetV2,Xception\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9b9e5ab-f4d4-48ba-84fc-58f88b1bc577",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model=MobileNetV2(input_shape=(160,160,3),           \n",
    "                    include_top=False,\n",
    "                    weights='imagenet') \n",
    "\n",
    "for layer in trained_model.layers:\n",
    "    layer.trainable=False\n",
    "#trained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3b83e5d-f96d-4fb7-8aa4-5f2c5c42a1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version +978d2ce)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "seed = 42\n",
    "env.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "#env._max_episode_steps = 1000\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "def create_model(mod):\n",
    "  inputs = layers.Input(shape=(160,160,3))\n",
    "  x = mod(inputs,training=False)\n",
    "  x1 = layers.GlobalAveragePooling2D()(x)\n",
    "  actor = layers.Dense(env.action_space.n,activation='linear')(x1)\n",
    "  #critic = layers.Dense(1,activation = 'linear')(x1)\n",
    "\n",
    "  return keras.Model(inputs=inputs, outputs=actor)\n",
    "\n",
    "model = create_model(trained_model)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),loss=\"mean_squared_error\")\n",
    "target_model = create_model(trained_model)\n",
    "target_model.compile(optimizer=Adam(learning_rate=0.001),loss=\"mean_squared_error\")\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6d53efc-a263-450c-aa98-84d5e4bab82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "\n",
    "  r = np.array(r)\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  gamma = 0.99\n",
    "\n",
    "  for t in range(r.size-2,-1,-1):\n",
    "    discounted_r[t] = r[t]+discounted_r[t+1]*gamma\n",
    "  discounted_r -= np.mean(discounted_r) #normalizing the result\n",
    "  discounted_r /= (np.std(discounted_r)+eps) #idem using standar deviation\n",
    "  return discounted_r.astype(np.float32)\n",
    "\n",
    "def prepro(k):\n",
    "    k=k[:160,:,:]\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca5d65-06c3-465e-a593-8e512d3c6878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode reward: -21.0 mean reward 0 episode: 0 frame: 1319\n",
      "episode reward: -20.0 mean reward -21.0 episode: 1 frame: 2821\n",
      "episode reward: -20.0 mean reward -20.5 episode: 2 frame: 4188\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_rewards = 0\n",
    "frame_count = 0\n",
    "episode_count = 0\n",
    "max_memory_length = 100000\n",
    "update_after = 4\n",
    "update_target_after = 1000\n",
    "epsilon = 1\n",
    "gamma = 0.99\n",
    "\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "action_taken=[]\n",
    "prev = None\n",
    "flag=0\n",
    "for i in range(100):\n",
    "  state = np.array(env.reset())\n",
    "  rewards=0\n",
    "\n",
    "  for step in range(1,10000):\n",
    "    frame_count+=1\n",
    "\n",
    "    if frame_count < 80000 or epsilon > np.random.rand(1)[0]:\n",
    "      action = np.random.choice(6)\n",
    "      state = prepro(state)\n",
    "    else:\n",
    "      state = prepro(state)\n",
    "      state_tensor = tf.convert_to_tensor(state)\n",
    "      state_tensor = tf.expand_dims(state_tensor,0)\n",
    "      action_prob = model.predict(state_tensor)\n",
    "      action = tf.random.categorical(action_prob, 1)[0, 0]\n",
    "      action = action.numpy()\n",
    "\n",
    "    epsilon -= 0.99/100000\n",
    "    epsilon = max(epsilon,0.1)\n",
    "\n",
    "    state_next,reward,done,info = env.step(action)\n",
    "    state_next = np.array(state_next)\n",
    "    state_next1 = prepro(state_next)\n",
    "\n",
    "    rewards += reward\n",
    "\n",
    "    action_history.append(action)\n",
    "    state_history.append(state)\n",
    "    state_next_history.append(state_next1)\n",
    "    done_history.append(done)\n",
    "    rewards_history.append(reward)\n",
    "\n",
    "    state = state_next\n",
    "\n",
    "    if frame_count % 4 == 0 and len(done_history) > 32:\n",
    "      indices = np.random.choice(range(len(done_history)),size=32)\n",
    "\n",
    "      state_sample = np.array([state_history[i] for i in indices])\n",
    "      state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "      rewards_sample = [rewards_history[i] for i in indices]\n",
    "      action_sample = [action_history[i] for i in indices]\n",
    "      done_sample = tf.convert_to_tensor([float(done_history[i]) for i in indices])\n",
    "\n",
    "      future_rewards = target_model.predict(state_next_sample)\n",
    "      current_rewards = model.predict(state_sample)\n",
    "      updated_q = current_rewards[:]\n",
    "\n",
    "      a = [i for i in range(6)]\n",
    "      for idx,terminal in enumerate(done_sample):\n",
    "        if terminal:\n",
    "          future_rewards[idx]=0.0\n",
    "        updated_q[idx,action_sample[idx]] = rewards_sample[idx] + gamma*tf.reduce_max(future_rewards,axis=1)[idx]\n",
    "\n",
    "      model.train_on_batch(state_sample,updated_q)\n",
    "\n",
    "    if frame_count % update_target_after ==0:\n",
    "      target_model.set_weights(model.get_weights())\n",
    "      #print(\"reward\",total_rewards,\"episode:\",episode_count,\"frame:\",frame_count)\n",
    "\n",
    "    if len(rewards_history)>max_memory_length:\n",
    "      del rewards_history[:1]\n",
    "      del state_history[:1]\n",
    "      del state_next_history[:1]\n",
    "      del action_history[:1]\n",
    "      del done_history[:1]\n",
    "\n",
    "    if done:\n",
    "      break\n",
    "\n",
    "  print(\"episode reward:\",rewards,\"mean reward\",total_rewards,\"episode:\",episode_count,\"frame:\",frame_count)\n",
    "  episode_reward_history.append(rewards)\n",
    "  if len(episode_reward_history)>100:\n",
    "    del episode_reward_history[:1]\n",
    "  total_rewards = np.mean(episode_reward_history)\n",
    "\n",
    "  episode_count+=1\n",
    "    \n",
    "  if episode_count%20==0:\n",
    "    !rm -rf ./saved_models/\n",
    "    !mkdir saved_models\n",
    "    model.save(f'./saved_models/model_{episode_count}.h5')\n",
    "    target_model.save(f'./saved_models/target_{episode_count}.h5')\n",
    "    d = {'ac':action_history,'st':state_history,'stn':state_next_history,\n",
    "         'r':reward_history,'d':done_history,'epr':episode_reward_history,\n",
    "         'at':action_taken,'ep':epsilon}\n",
    "    file = open('history','wb')\n",
    "    pickle.dump(d,file)\n",
    "    file.close()\n",
    "\n",
    "  if total_rewards > -5:\n",
    "    print(\"solved at episode: \",episode_count)\n",
    "    break\n",
    "#env.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380824f8-29f8-402e-a936-fc7cc4680bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
